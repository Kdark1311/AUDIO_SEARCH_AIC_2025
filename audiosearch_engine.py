"""
ELASTICSEARCH SPEECH RETRIEVAL SYSTEM - FINAL VERSION
vá»›i Vietnamese Plugin + Fuzzy Search + Incremental Indexing
Compatible with Elasticsearch 8.7.0 + elasticsearch-analysis-vietnamese plugin

TÃ­nh nÄƒng:
âœ… Vietnamese Plugin Analyzer (tÃ¡ch tá»« tiáº¿ng Viá»‡t chuyÃªn nghiá»‡p)
âœ… Fuzzy Search (cho phÃ©p gÃµ sai 1-2 kÃ½ tá»±)
âœ… Incremental Indexing (bá» qua file Ä‘Ã£ index)
âœ… BM25 + Vector Search
âœ… 4 cháº¿ Ä‘á»™ sá»­ dá»¥ng linh hoáº¡t
"""

import os
import json
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from typing import List, Dict
import warnings
import hashlib
warnings.filterwarnings('ignore')


# ========================== HELPER FUNCTIONS ==========================
def list_keyframes_in_range(entry: Dict, base_keyframe_dir: str) -> List[str]:
    """
    Láº¥y danh sÃ¡ch frame thá»±c táº¿ trong thÆ° má»¥c Keyframes
    mÃ  cÃ³ sá»‘ frame náº±m trong [start_frame, end_frame].    
    """
    json_file = os.path.basename(entry["file"])
    video_name = os.path.splitext(json_file)[0]
    k_folder = video_name.split('_')[0]  # K01
    keyframe_folder = os.path.join(base_keyframe_dir, k_folder, video_name)
    
    start_f, end_f = entry["start_frame"], entry["end_frame"]
    frame_paths = []

    if os.path.exists(keyframe_folder):
        for fname in sorted(os.listdir(keyframe_folder)):
            if fname.endswith((".webp", ".jpg", ".png")):
                try:
                    frame_num = int(os.path.splitext(fname)[0])
                    if start_f <= frame_num <= end_f:
                        frame_paths.append(os.path.join(keyframe_folder, fname))
                except ValueError:
                    continue
    
    return frame_paths


def get_file_hash(filepath: str) -> str:
    """
    TÃ­nh hash cá»§a file Ä‘á»ƒ phÃ¡t hiá»‡n thay Ä‘á»•i
    """
    hash_md5 = hashlib.md5()
    try:
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception:
        return ""


# ========================== MAIN CLASS ==========================
class SpeechRetrievalES:
    """
    Speech Retrieval System using Elasticsearch
    
    Features:
    - Vietnamese Plugin Analyzer (tÃ¡ch tá»« tiáº¿ng Viá»‡t chuyÃªn nghiá»‡p)
    - Keyword search (BM25) with Fuzzy matching
    - Semantic search (Dense Vector)
    - Keyframe extraction
    - Incremental indexing (skip already indexed files)
    """
    
    def __init__(
        self,
        context_json_dir: str,
        base_keyframe_dir: str,
        host: str = "http://localhost:9200",
        index_name: str = "speech_index",
        semantic_model: str = "sentence-transformers/stsb-xlm-r-multilingual",
        use_semantic: bool = True,
        load_data: bool = True,
        force_reindex: bool = False,  # ğŸ†• CÆ°á»¡ng cháº¿ index láº¡i táº¥t cáº£
        index_tracker_file: str = ".indexed_files.json"  # ğŸ†• File lÆ°u danh sÃ¡ch Ä‘Ã£ index
    ):
        """
        Khá»Ÿi táº¡o Speech Retrieval System
        
        Args:
            context_json_dir: ThÆ° má»¥c chá»©a cÃ¡c file JSON transcript
            base_keyframe_dir: ThÆ° má»¥c chá»©a keyframes
            host: Elasticsearch host URL
            index_name: TÃªn index trong Elasticsearch
            semantic_model: Model cho semantic search
            use_semantic: CÃ³ sá»­ dá»¥ng semantic search khÃ´ng
            load_data: CÃ³ load dá»¯ liá»‡u vÃ o ES khÃ´ng (False náº¿u Ä‘Ã£ index rá»“i)
            force_reindex: True = index láº¡i táº¥t cáº£ (bá» qua tracking)
            index_tracker_file: File JSON lÆ°u danh sÃ¡ch file Ä‘Ã£ index
        """
        self.context_json_dir = context_json_dir
        self.base_keyframe_dir = base_keyframe_dir
        self.index_name = index_name
        self.use_semantic = use_semantic
        self.force_reindex = force_reindex
        self.index_tracker_file = index_tracker_file

        print("="*80)
        print("ğŸš€ KHá»I Äá»˜NG SPEECH RETRIEVAL SYSTEM")
        print("="*80)
        print("âœ¨ Vietnamese Plugin + Fuzzy Search + Incremental Indexing")
        
        # Load danh sÃ¡ch file Ä‘Ã£ index
        self.indexed_files = self._load_indexed_files()
        
        # Káº¿t ná»‘i Elasticsearch
        self._connect_elasticsearch(host)
        
        # Load semantic model
        if use_semantic:
            self._load_semantic_model(semantic_model)
        
        # Táº¡o index
        self._setup_index()
        
        # Index dá»¯ liá»‡u
        if load_data:
            self._index_data()

    def _load_indexed_files(self) -> Dict[str, str]:
        """
        Load danh sÃ¡ch file Ä‘Ã£ index tá»« file JSON
        Returns: Dict {filepath: hash}
        """
        if self.force_reindex:
            print("\nâš ï¸  Force reindex enabled - sáº½ index láº¡i táº¥t cáº£ file")
            return {}
        
        if os.path.exists(self.index_tracker_file):
            try:
                with open(self.index_tracker_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f"\nğŸ“‹ ÄÃ£ load thÃ´ng tin {len(data)} file Ä‘Ã£ index tá»« {self.index_tracker_file}")
                return data
            except Exception as e:
                print(f"\nâš ï¸  KhÃ´ng thá»ƒ Ä‘á»c {self.index_tracker_file}: {e}")
                return {}
        else:
            print(f"\nğŸ“‹ File tracker chÆ°a tá»“n táº¡i - sáº½ táº¡o má»›i")
            return {}

    def _save_indexed_files(self):
        """
        LÆ°u danh sÃ¡ch file Ä‘Ã£ index vÃ o file JSON
        """
        try:
            with open(self.index_tracker_file, 'w', encoding='utf-8') as f:
                json.dump(self.indexed_files, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ÄÃ£ lÆ°u thÃ´ng tin {len(self.indexed_files)} file vÃ o {self.index_tracker_file}")
        except Exception as e:
            print(f"\nâš ï¸  KhÃ´ng thá»ƒ lÆ°u {self.index_tracker_file}: {e}")

    def _should_index_file(self, filepath: str) -> bool:
        """
        Kiá»ƒm tra xem file cÃ³ cáº§n index khÃ´ng
        Returns: True náº¿u cáº§n index (file má»›i hoáº·c Ä‘Ã£ thay Ä‘á»•i)
        """
        if self.force_reindex:
            return True
        
        # TÃ­nh hash cá»§a file hiá»‡n táº¡i
        current_hash = get_file_hash(filepath)
        
        # Kiá»ƒm tra file Ä‘Ã£ Ä‘Æ°á»£c index chÆ°a
        if filepath in self.indexed_files:
            stored_hash = self.indexed_files[filepath]
            # Náº¿u hash giá»‘ng nhau = file khÃ´ng Ä‘á»•i = khÃ´ng cáº§n index láº¡i
            if stored_hash == current_hash:
                return False
        
        return True

    def _connect_elasticsearch(self, host: str):
        """Káº¿t ná»‘i tá»›i Elasticsearch"""
        print(f"\nğŸ”Œ Äang káº¿t ná»‘i tá»›i {host}...")
        
        try:
            from elasticsearch import __version__ as es_version
            print(f"   PhiÃªn báº£n client: {es_version}")
            
            # Táº¡o connection
            self.es = Elasticsearch(
                hosts=[host],
                verify_certs=False,
                ssl_show_warn=False,
                request_timeout=30,
                max_retries=3,
                retry_on_timeout=True
            )
            
            # Kiá»ƒm tra káº¿t ná»‘i
            info = self.es.info()
            print(f"âœ… Káº¿t ná»‘i thÃ nh cÃ´ng!")
            print(f"   - Cluster: {info['cluster_name']}")
            print(f"   - Version: {info['version']['number']}")
            
        except Exception as e:
            print(f"\nâŒ Lá»–I Káº¾T Ná»I: {e}")
            print("\nğŸ”§ HÆ°á»›ng dáº«n kháº¯c phá»¥c:")
            print("1. Kiá»ƒm tra Docker container: docker ps")
            print("2. Xem logs: docker logs elasticsearch")
            print("3. Test curl: curl http://localhost:9200")
            print("4. Kiá»ƒm tra version: pip show elasticsearch")
            raise ConnectionError("KhÃ´ng thá»ƒ káº¿t ná»‘i tá»›i Elasticsearch!")

    def _load_semantic_model(self, model_name: str):
        """Load sentence transformer model"""
        print(f"\nğŸ“¥ Äang táº£i model: {model_name}...")
        self.model = SentenceTransformer(model_name)
        print("âœ… Model Ä‘Ã£ sáºµn sÃ ng!")

    def _setup_index(self):
        """
        âœ¨ Táº¡o index vá»›i Vietnamese Plugin Analyzer
        
        Plugin 'vietnamese' cung cáº¥p:
        - Vietnamese tokenizer: TÃ¡ch tá»« tiáº¿ng Viá»‡t chÃ­nh xÃ¡c
        - Stop words: Tá»± Ä‘á»™ng loáº¡i bá» tá»« vÃ´ nghÄ©a
        - Normalization: Chuáº©n hÃ³a text tiáº¿ng Viá»‡t
        """
        print(f"\nğŸ”§ Kiá»ƒm tra index '{self.index_name}'...")
        
        try:
            if self.es.indices.exists(index=self.index_name):
                print(f"â„¹ï¸  Index Ä‘Ã£ tá»“n táº¡i")
                return
            
            print(f"ğŸ”¨ Táº¡o index má»›i vá»›i Vietnamese Plugin...")
            
            # âœ¨ Sá»­ dá»¥ng analyzer "vietnamese" tá»« plugin
            mapping = {
                "mappings": {
                    "properties": {
                        "text": {
                            "type": "text",
                            "analyzer": "vietnamese"  # ğŸ‘ˆ Analyzer tá»« plugin
                        },
                        "start_frame": {"type": "integer"},
                        "end_frame": {"type": "integer"},
                        "start_sec": {"type": "float"},
                        "end_sec": {"type": "float"},
                        "file": {"type": "keyword"},
                        "video_name": {"type": "keyword"},
                        "L": {"type": "integer"},
                        "embedding": {
                            "type": "dense_vector",
                            "dims": 768,
                            "index": True,
                            "similarity": "cosine"
                        }
                    }
                }
            }
            
            self.es.indices.create(index=self.index_name, body=mapping)
            print("âœ… Index Ä‘Ã£ Ä‘Æ°á»£c táº¡o vá»›i Vietnamese Plugin!")
            print("   - Tokenizer: Vietnamese word segmentation")
            print("   - Analyzer: Optimized for Vietnamese text")
            
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi táº¡o index: {e}")
            if "vietnamese" in str(e).lower():
                print("\nâŒ PLUGIN CHÆ¯A ÄÆ¯á»¢C CÃ€I Äáº¶T!")
                print("Plugin Ä‘Ã£ cÃ i chÆ°a? Kiá»ƒm tra:")
                print("  docker exec -it elasticsearch bin/elasticsearch-plugin list")
            raise

    def _index_data(self):
        """Äá»c vÃ  index dá»¯ liá»‡u tá»« cÃ¡c file JSON (chá»‰ index file má»›i/thay Ä‘á»•i)"""
        print("\n" + "="*80)
        print("ğŸ“‚ Báº®T Äáº¦U INDEX Dá»® LIá»†U (INCREMENTAL)")
        print("="*80)
        print(f"ThÆ° má»¥c: {self.context_json_dir}\n")
        
        # Äáº¿m sá»‘ file
        all_files = []
        for root, _, files in os.walk(self.context_json_dir):
            for file in files:
                if file.endswith(".json"):
                    all_files.append(os.path.join(root, file))
        
        print(f"ğŸ“Š Tá»•ng sá»‘ file JSON: {len(all_files)}")
        print(f"ğŸ“‹ Sá»‘ file Ä‘Ã£ index trÆ°á»›c Ä‘Ã³: {len(self.indexed_files)}")
        
        # Lá»c file cáº§n index
        files_to_index = [f for f in all_files if self._should_index_file(f)]
        files_skipped = len(all_files) - len(files_to_index)
        
        print(f"ğŸ†• File cáº§n index: {len(files_to_index)}")
        print(f"â­ï¸  File bá» qua (Ä‘Ã£ index): {files_skipped}")
        
        if len(files_to_index) == 0:
            print("\nâœ… KhÃ´ng cÃ³ file má»›i - Bá» qua indexing")
            return
        
        print()
        
        count = 0
        indexed_count = 0
        
        for full_path in files_to_index:
            video_name = os.path.splitext(os.path.basename(full_path))[0]
            
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                
                file_display = os.path.basename(full_path)
                print(f"ğŸ“„ {file_display} ({len(data)} entries)")
                
                # Index tá»«ng entry vá»›i progress bar
                for idx, item in enumerate(tqdm(data, desc=f"  Indexing", leave=False)):
                    text = item.get("text", "").strip()
                    if not text:
                        continue
                    
                    doc = {
                        "text": text,
                        "start_frame": item.get("start_frame"),
                        "end_frame": item.get("end_frame"),
                        "start_sec": item.get("start_sec"),
                        "end_sec": item.get("end_sec"),
                        "file": full_path,
                        "video_name": video_name,
                        "L": idx,
                    }

                    # ThÃªm embedding
                    if self.use_semantic:
                        doc["embedding"] = self.model.encode(text).tolist()

                    self.es.index(index=self.index_name, document=doc)
                    count += 1
                
                # Cáº­p nháº­t hash cá»§a file vÃ o tracker
                self.indexed_files[full_path] = get_file_hash(full_path)
                indexed_count += 1
                
                print(f"   âœ… ÄÃ£ index xong\n")
                
            except Exception as e:
                print(f"   âš ï¸  Lá»—i: {e}\n")
        
        # LÆ°u danh sÃ¡ch file Ä‘Ã£ index
        self._save_indexed_files()
        
        print("="*80)
        print(f"ğŸ‰ HOÃ€N THÃ€NH!")
        print(f"   - ÄÃ£ index: {indexed_count} file")
        print(f"   - Tá»•ng documents: {count:,}")
        print(f"   - Bá» qua: {files_skipped} file (Ä‘Ã£ index trÆ°á»›c Ä‘Ã³)")
        print("="*80)

    def search(self, query: str, k: int = 3, use_fuzzy: bool = False) -> Dict:
        """
        âœ¨ TÃ¬m kiáº¿m vá»›i Vietnamese Plugin + Fuzzy Search
        
        Args:
            query: CÃ¢u truy váº¥n
            k: Sá»‘ káº¿t quáº£ tráº£ vá»
            use_fuzzy: Báº­t fuzzy search (cho phÃ©p gÃµ sai)
            
        Returns:
            Dict chá»©a káº¿t quáº£ keyword vÃ  semantic
        """
        results = {}

        # 1. Keyword Search (BM25) vá»›i Fuzzy option
        if use_fuzzy:
            # âœ¨ FUZZY SEARCH - Cho phÃ©p gÃµ sai 1-2 kÃ½ tá»±
            keyword_query = {
                "size": k,
                "query": {
                    "match": {
                        "text": {
                            "query": query,
                            "fuzziness": "AUTO",  # Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh (1-2 kÃ½ tá»± sai)
                            "prefix_length": 1,    # Ãt nháº¥t 1 kÃ½ tá»± Ä‘áº§u Ä‘Ãºng
                            "max_expansions": 50   # Tá»‘i Ä‘a 50 biáº¿n thá»ƒ
                        }
                    }
                }
            }
        else:
            # Keyword search thÃ´ng thÆ°á»ng
            keyword_query = {
                "size": k,
                "query": {"match": {"text": query}}
            }
        
        resp = self.es.search(index=self.index_name, body=keyword_query)
        results["keyword"] = [hit["_source"] for hit in resp["hits"]["hits"]]

        # 2. Semantic Search (Dense Vector)
        if self.use_semantic:
            query_vec = self.model.encode(query).tolist()
            
            semantic_query = {
                "size": k,
                "knn": {
                    "field": "embedding",
                    "query_vector": query_vec,
                    "k": k,
                    "num_candidates": 100
                }
            }
            
            try:
                resp = self.es.search(index=self.index_name, **semantic_query)
                results["semantic"] = [hit["_source"] for hit in resp["hits"]["hits"]]
            except Exception as e:
                print(f"âš ï¸  Lá»—i semantic search: {e}")
                results["semantic"] = []

        return results

    def search_with_frames(self, query: str, k: int = 3, use_fuzzy: bool = False) -> Dict:
        """
        TÃ¬m kiáº¿m vÃ  kÃ¨m theo keyframes
        
        Args:
            query: CÃ¢u truy váº¥n
            k: Sá»‘ káº¿t quáº£ tráº£ vá»
            use_fuzzy: Báº­t fuzzy search
            
        Returns:
            Dict chá»©a káº¿t quáº£ search + danh sÃ¡ch keyframes
        """
        results = self.search(query, k, use_fuzzy)
        output = {}

        for mode in ["semantic", "keyword"]:
            output[mode] = []
            for r in results.get(mode, []):
                frames = list_keyframes_in_range(r, self.base_keyframe_dir)
                r["frames"] = frames
                r["num_frames"] = len(frames)
                output[mode].append(r)
        
        return output

    def display_results(self, results: Dict):
        """
        Hiá»ƒn thá»‹ káº¿t quáº£ search má»™t cÃ¡ch Ä‘áº¹p máº¯t
        
        Args:
            results: Dict chá»©a káº¿t quáº£ tá»« search_with_frames()
        """
        for mode in ["semantic", "keyword"]:
            print(f"\n{'='*80}")
            print(f"ğŸ” {mode.upper()} SEARCH RESULTS")
            print('='*80)
            
            mode_results = results.get(mode, [])
            if not mode_results:
                print("âŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£")
                continue
            
            for i, r in enumerate(mode_results, 1):
                print(f"\n[{i}] Video: {r.get('video_name', 'N/A')}")
                print(f"    ğŸ“ Text: {r['text'][:150]}...")
                print(f"    â±ï¸  Time: {r['start_sec']:.2f}s â†’ {r['end_sec']:.2f}s")
                print(f"    ğŸï¸  Frames: {r['start_frame']} â†’ {r['end_frame']}")
                print(f"    ğŸ“¸ Keyframes: {r.get('num_frames', 0)} frames")
                
                if r.get('frames'):
                    print(f"    ğŸ“ Danh sÃ¡ch keyframes:")
                    for frame in r['frames'][:5]:
                        print(f"       â€¢ {frame}")
                    if len(r['frames']) > 5:
                        print(f"       ... vÃ  {len(r['frames']) - 5} frames khÃ¡c")

    def reset_index_tracker(self):
        """
        ğŸ—‘ï¸ XÃ“A file tracker - dÃ¹ng khi muá»‘n index láº¡i tá»« Ä‘áº§u
        """
        if os.path.exists(self.index_tracker_file):
            os.remove(self.index_tracker_file)
            print(f"ğŸ—‘ï¸  ÄÃ£ xÃ³a {self.index_tracker_file}")
            self.indexed_files = {}
        else:
            print(f"â„¹ï¸  File {self.index_tracker_file} khÃ´ng tá»“n táº¡i")


# ========================== MAIN PROGRAM ==========================
if __name__ == "__main__":
    # Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n
    AUDIO_DIR = r"E:\DATA\AUDIO_RECOGNIZATION"
    KEYFRAME_DIR = r"E:\DATA\AIC_2025"
    
    # ğŸ¯ CÃC CHáº¾ Äá»˜ Sá»¬ Dá»¤NG:
    
    # ============ CHáº¾ Äá»˜ 1: Láº¦N Äáº¦U TIÃŠN (Index ) ============
    retrieval = SpeechRetrievalES(
        context_json_dir=AUDIO_DIR,
        base_keyframe_dir=KEYFRAME_DIR,
        host="http://localhost:9200",
        index_name="speech_index_vn",
        use_semantic=True,
        load_data=True,
        force_reindex=False  # False = chá»‰ index file má»›i
    )
    

    
    # ============ CHáº¾ Äá»˜ 3: CHá»ˆ TÃŒM KIáº¾M (KhÃ´ng index) ============
    # retrieval = SpeechRetrievalES(
    #     context_json_dir=AUDIO_DIR,
    #     base_keyframe_dir=KEYFRAME_DIR,
    #     host="http://localhost:9200",
    #     index_name="speech_index_vn",
    #     use_semantic=True,
    #     load_data=False  # False = khÃ´ng index, chá»‰ search
    # )
    
    # # ========================== DEMO TÃŒM KIáº¾M ==========================
    # print("\n" + "="*80)
    # print("ğŸ” DEMO TÃŒM KIáº¾M")
    # print("="*80)
    
    # # VÃ­ dá»¥ 1: Search thÃ´ng thÆ°á»ng
    # query1 = "xe Ã´ tÃ´"
    # print(f"\nQuery: '{query1}' (khÃ´ng fuzzy)")
    # results1 = retrieval.search_with_frames(query1, k=5, use_fuzzy=False)
    # retrieval.display_results(results1)
    
    # VÃ­ dá»¥ 2: Search vá»›i fuzzy
    # query2 = "oto"
    # print(f"\nQuery: '{query2}' (vá»›i fuzzy - gÃµ sai)")
    # results2 = retrieval.search_with_frames(query2, k=5, use_fuzzy=True)
    # retrieval.display_results(results2)
    
    # print("\n" + "="*80)
    # print("âœ… HOÃ€N THÃ€NH!")
    # print("="*80)
    
    # ğŸ’¡ TIP: Náº¿u muá»‘n xÃ³a tracker vÃ  index láº¡i tá»« Ä‘áº§u:
    # retrieval.reset_index_tracker()
"""
ELASTICSEARCH IMPORT SCRIPT - IMPROVED VERSION
Import data tá»« exported files (clone tá»« GitHub)
Compatible with Elasticsearch 8.11.0
"""

import os
import json
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from sentence_transformers import SentenceTransformer
from tqdm import tqdm


def import_index_from_files(
    host="http://localhost:9200",
    input_dir="elasticsearch_data",
    regenerate_embeddings=True,
    semantic_model="sentence-transformers/stsb-xlm-r-multilingual"
):
    """
    Import Elasticsearch index tá»« exported files
    
    Args:
        host: Elasticsearch URL
        input_dir: ThÆ° má»¥c chá»©a exported files (clone tá»« GitHub)
        regenerate_embeddings: Táº¡o láº¡i embeddings (khuyÃªn dÃ¹ng=True)
        semantic_model: Model Ä‘á»ƒ táº¡o embeddings
    """
    
    print("="*80)
    print("ğŸ“¦ ELASTICSEARCH INDEX IMPORT")
    print("="*80)
    
    # Kiá»ƒm tra thÆ° má»¥c input
    if not os.path.exists(input_dir):
        print(f"âŒ ThÆ° má»¥c '{input_dir}' khÃ´ng tá»“n táº¡i!")
        print("   HÃ£y Ä‘áº£m báº£o Ä‘Ã£ clone repo tá»« GitHub")
        return
    
    # Káº¿t ná»‘i ES
    print(f"\nğŸ”Œ Äang káº¿t ná»‘i tá»›i {host}...")
    es = Elasticsearch(
        hosts=[host], 
        verify_certs=False, 
        ssl_show_warn=False,
        request_timeout=60,
        max_retries=3,
        retry_on_timeout=True
    )
    
    if not es.ping():
        print("âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i Elasticsearch!")
        print("   HÃ£y cháº¡y: ./setup_elasticsearch.sh")
        return
    
    print("âœ… ÄÃ£ káº¿t ná»‘i")
    
    # Load metadata
    metadata_file = os.path.join(input_dir, "metadata.json")
    if not os.path.exists(metadata_file):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y metadata.json trong {input_dir}")
        return
    
    with open(metadata_file, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    index_name = metadata['index_name']
    total_docs = metadata['total_documents']
    total_batches = metadata['total_batches']
    
    print(f"\nğŸ“‹ Metadata:")
    print(f"   - Index name: {index_name}")
    print(f"   - Total documents: {total_docs:,}")
    print(f"   - Total batches: {total_batches}")
    
    # Load mapping
    mapping_file = os.path.join(input_dir, "mapping.json")
    with open(mapping_file, 'r', encoding='utf-8') as f:
        mapping_data = json.load(f)
    
    # Láº¥y mapping cá»§a index (bá» qua index name cÅ©)
    original_mapping = list(mapping_data.values())[0]['mappings']
    
    # XÃ³a index cÅ© náº¿u tá»“n táº¡i
    if es.indices.exists(index=index_name):
        print(f"\nâš ï¸  Index '{index_name}' Ä‘Ã£ tá»“n táº¡i")
        response = input("XÃ³a vÃ  táº¡o láº¡i? (yes/no): ")
        if response.lower() != 'yes':
            print("âŒ ÄÃ£ há»§y")
            return
        es.indices.delete(index=index_name)
        print(f"ğŸ—‘ï¸  ÄÃ£ xÃ³a index cÅ©")
    
    # Táº¡o index má»›i vá»›i mapping (ES 8.x style)
    print(f"\nğŸ”¨ Äang táº¡o index '{index_name}'...")
    es.indices.create(index=index_name, mappings=original_mapping)  # âœ… mappings= not body=
    print("âœ… ÄÃ£ táº¡o index")
    
    # Load model náº¿u cáº§n regenerate embeddings
    model = None
    if regenerate_embeddings:
        print(f"\nğŸ“¥ Äang táº£i model: {semantic_model}...")
        model = SentenceTransformer(semantic_model)
        print("âœ… Model Ä‘Ã£ sáºµn sÃ ng")
    
    # Import documents vá»›i BULK API (nhanh hÆ¡n)
    print(f"\nğŸ“¤ Äang import documents (bulk mode)...")
    
    imported = 0
    errors = 0
    pbar = tqdm(total=total_docs, desc="Importing", unit="docs")
    
    for batch_num in range(total_batches):
        batch_file = os.path.join(input_dir, f"batch_{batch_num:04d}.json")
        
        if not os.path.exists(batch_file):
            print(f"\nâš ï¸  KhÃ´ng tÃ¬m tháº¥y {batch_file}")
            continue
        
        # Load batch
        with open(batch_file, 'r', encoding='utf-8') as f:
            batch_data = json.load(f)
        
        # Prepare bulk actions
        actions = []
        for doc in batch_data:
            # Regenerate embedding náº¿u cáº§n
            if regenerate_embeddings and model and doc.get('embedding') is None:
                text = doc.get('text', '')
                if text:
                    doc['embedding'] = model.encode(text).tolist()
            
            # Bulk action
            actions.append({
                '_index': index_name,
                '_source': doc
            })
        
        # Bulk index (nhanh hÆ¡n nhiá»u)
        try:
            success, failed = bulk(es, actions, raise_on_error=False)
            imported += success
            errors += len(failed)
            pbar.update(success)
            
            if failed:
                print(f"\nâš ï¸  Batch {batch_num}: {len(failed)} documents failed")
        except Exception as e:
            print(f"\nâŒ Lá»—i batch {batch_num}: {e}")
            errors += len(actions)
    
    pbar.close()
    
    # Refresh index
    es.indices.refresh(index=index_name)
    
    # Verify
    final_count = es.count(index=index_name)['count']
    
    print("\n" + "="*80)
    print("âœ… IMPORT HOÃ€N THÃ€NH!")
    print("="*80)
    print(f"ğŸ“Š Documents imported: {imported:,}")
    print(f"ğŸ“Š Documents in index: {final_count:,}")
    print(f"âŒ Errors: {errors:,}")
    
    if final_count != total_docs:
        print(f"\nâš ï¸  Cáº¢NH BÃO: Sá»‘ lÆ°á»£ng khÃ´ng khá»›p!")
        print(f"   Expected: {total_docs:,}")
        print(f"   Got: {final_count:,}")
        print(f"   Missing: {total_docs - final_count:,}")
    else:
        print("\nâœ… Sá»‘ lÆ°á»£ng documents khá»›p hoÃ n toÃ n!")
    
    print("\nğŸ’¡ BÆ°á»›c tiáº¿p theo:")
    print(f"   python speech_retrieval_interactive.py")
    print("="*80)


if __name__ == "__main__":
    # Cáº¥u hÃ¬nh
    HOST = "http://localhost:9200"
    INPUT_DIR = "elasticsearch_data"  # ThÆ° má»¥c clone tá»« GitHub
    
    print("\nğŸ¯ CÃC TÃ™Y CHá»ŒN:")
    print("1. Import vá»›i regenerate embeddings (khuyÃªn dÃ¹ng)")
    print("2. Import khÃ´ng regenerate embeddings (nhanh hÆ¡n)")
    
    choice = input("\nNháº­p lá»±a chá»n (1/2, Enter=1): ").strip()
    
    regenerate = True if choice != '2' else False
    
    if regenerate:
        print("\nâœ¨ Sáº½ táº¡o láº¡i embeddings tá»« text")
    else:
        print("\nâš¡ Sáº½ bá» qua embeddings (semantic search khÃ´ng hoáº¡t Ä‘á»™ng)")
    
    # Cháº¡y import
    import_index_from_files(
        host=HOST,
        input_dir=INPUT_DIR,
        regenerate_embeddings=regenerate
    )